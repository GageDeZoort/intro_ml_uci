{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ls6d5B5VSU66",
        "Z_zR-QZgSY5y",
        "v8Mbx1aAUTvK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI Introduction to Machine Learning\n",
        "**Day 3: Convolutional Neural Networks**\n",
        "\n",
        "\n",
        "Notebook adapted by Gage DeZoort from a similar notebook offered in Princeton University's [Introduction to Machine Learning Wintersession course](https://github.com/PrincetonUniversity/intro_machine_learning/tree/main).\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/GageDeZoort/intro_ml_uci/blob/main/day_2/intro_to_ML_day_3_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "zpCkWYWdmgbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The CIFAR-10 Dataset\n"
      ],
      "metadata": {
        "id": "ls6d5B5VSU66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook is to demo image classification in the **CIFAR10** dataset using **convolutional neural networks (CNNs)**.\n",
        "\n",
        "If you'd like to use a GPU, click on `Runtime` (in the Colab toolbar above) then `Change runtime type` and select `T4 GPU`.\n"
      ],
      "metadata": {
        "id": "2YtdceqGWzdv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LzSJGvKV7JE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" # or \"cuda\" if you want to use the GPU!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [**CIFAR-10 dataset**](https://www.cs.toronto.edu/~kriz/cifar.html) is a classic benchmark for image classification.  \n",
        "“CIFAR” stands for **Canadian Institute for Advanced Research**, and “10” refers to its **10 image categories**.\n",
        "\n",
        "---\n",
        "\n",
        "**Key properties:**\n",
        "- **10 classes:** airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck  \n",
        "- **6,000 images per class** → **60,000 images total**  \n",
        "- **Image size:** 32×32 pixels, RGB color (so each image has shape **32×32×3**)  \n",
        "- Standard split: 50,000 for training, 10,000 for testing  \n",
        "\n",
        "---\n",
        "\n",
        "Each example in CIFAR-10 is a small, low-resolution color photo representing one of the 10 object categories.  \n",
        "We’ll train a neural network to recognize which class each image belongs to.\n",
        "\n",
        "Here’s a quick visual preview\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/GageDeZoort/intro_ml_uci/main/day_3/images/rgb_images.png\" width=\"300\">\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "EeR8TYPic_OJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**`torchvision`** is a companion library to [PyTorch](https://pytorch.org/) that provides tools, datasets, and models for **computer vision** tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Core modules include:**\n",
        "- `torchvision.datasets` → standardized datasets for quick loading  \n",
        "- `torchvision.transforms` → image preprocessing and augmentation tools  \n",
        "- `torchvision.models` → pretrained CNN and vision transformer architectures  \n",
        "- `torchvision.utils` → helper functions for visualizing and saving images  "
      ],
      "metadata": {
        "id": "r6OxpA3BRYm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform RGB features from [0,1]x3 to [-1,1]x3\n",
        "transform = transforms.Compose(\n",
        "  [ # image = (image - mean) / std\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          (0.5, 0.5, 0.5), # means\n",
        "          (0.5, 0.5, 0.5) # stds\n",
        "      )\n",
        "   ]\n",
        ")\n",
        "\n",
        "# grab the train and test sets\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        "  )\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "zUWR3zazYJE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the datasets loaded, we can use the `DataLoader` class provided by torch to conveniently load up random batches of data."
      ],
      "metadata": {
        "id": "JPu-y0Kdf-fE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "# feed the train/test sets into data loaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "zTZiRyjMf1sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our loaders in place, let's plot one batch of data."
      ],
      "metadata": {
        "id": "BG06xqjXgTNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "  img = img / 2 + 0.5  # unnormalize\n",
        "  plt.imshow(np.transpose(img.cpu().numpy(), (1, 2, 0)))\n",
        "  plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# truth labels\n",
        "classes = (\n",
        "    'plane', 'car', 'bird', 'cat', 'deer',\n",
        "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        ")\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ],
      "metadata": {
        "id": "iojIsXgTaoz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 1**: Print the shape of `images` above. Does it make sense?"
      ],
      "metadata": {
        "id": "NvG3GCAnSJPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 1\n"
      ],
      "metadata": {
        "id": "bvsoy_YxRwJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> If you only plot one feature per-pixel instead of 3, it will be interpreted as a greyscale image. Try plotting only one of the RGB features."
      ],
      "metadata": {
        "id": "CsnCJnEUT-IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 1\n"
      ],
      "metadata": {
        "id": "oe5O6oE9TiBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Image Classification with a CNN"
      ],
      "metadata": {
        "id": "Z_zR-QZgSY5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next goal is to define a CNN to do the image classification. The CNN will be a `torch.nn.Module`, which is a generic class designed to hold neural network components. The `forward()` function of the network is the forward pass; everything called there will be used to transform the input data (using learnable weights) and produce a prediction."
      ],
      "metadata": {
        "id": "DoBtx2ayg_5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional Layers**: Convolutional layers use **learnable filters** (also called **kernels**) to detect visual patterns in images. Each filter is a small matrix of **learnable weights** (e.g., 3x3 or 5x5) that slides across the image, computing the **elementwise product** of its weights and the underlying pixel values.\n",
        "\n",
        "During training, these filter weights are updated via backpropagation so they learn to respond to meaningful features — such as edges, textures, or object parts. Stacking many convolutional layers allows a network to build up increasingly abstract representations of an image, from low-level shapes to high-level concepts.\n",
        "\n",
        "There is a [wonderful Medium article](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) that visualizes the process. Convolutional filters extract high level information from each RGB channel: ![Conv](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*8dx6nxpUh2JqvYWPadTwMQ.gif \"conv\")\n",
        "\n",
        "Information extracted from each channel is summed at the output of each convolutional layer: ![ConvSum](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*CYB2dyR3EhFs1xNLK8ewiA.gif \"conv-sum\")\n",
        "\n",
        "The filters are **learned** by the algorithm; the intuition is that they become \"detectors\" for certain image features. For exmaple, here's a nice example of a kernel filter that detects vertical lines (sourced from the Medium article): ![EdgeDetector](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png \"edge-detector\")"
      ],
      "metadata": {
        "id": "8Ags5fltYkQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pooling Layers** are designed to downsample images. Contrary to filters, they are *not* learnable, instead representing fixed operations. For example, a pooling layer say \"take the maximum value from each 3x3 grid of cells\".\n",
        "![Pooling](https://miro.medium.com/v2/resize:fit:588/format:webp/1*BMngs93_rm2_BpJFH2mS0Q.gif \"pooling\")\n",
        "\n",
        "Pooling is used to aggregate information and shape it in a useful way for downstream learning tasks.\n"
      ],
      "metadata": {
        "id": "cK8b64Afyf7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you will see **fully-connected layers**;\n",
        "these are the simple PyTorch `linear` layers we saw yesteday. After convolutional filters and pooling layers have been applied, the resulting data is flattened and passed through a simple fully connected network to produce classification scores."
      ],
      "metadata": {
        "id": "_xuxOWh0ykAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input image shape: [batch_size, 3, 32, 32]\n",
        "        #   - 3 channels (RGB)\n",
        "        #   - 32x32 pixels\n",
        "\n",
        "        # Conv layer 1:\n",
        "        #   - 3 input channels → 6 output channels (6 learned filters)\n",
        "        #   - Each filter is 3x5x5, where the RGB dimension (3) is summed over\n",
        "        # Output shape after conv1: [batch_size, 6, 28, 28]\n",
        "        #   → 5x5 filter size narrows the input dimensions by 2\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1)\n",
        "\n",
        "        # Max pooling layer:\n",
        "        #   - 2x2 pooling with stride 2 → halves width and height\n",
        "        # Output shape after pool1: [batch_size, 6, 14, 14]\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Conv layer 2:\n",
        "        #   - 6 input channels → 16 output channels\n",
        "        #   - Each filter is 6x5x5, where the feature dimension (6) is summed over\n",
        "        # Output shape after conv2: [batch_size, 16, 10, 10]\n",
        "        #   → 5x5 filter narrows the input dimensions by 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "\n",
        "        # In the forward function, we apply the pooling operation again\n",
        "        # Second pooling → halves 10x10 → 5x5\n",
        "        # Output shape after pool2: [batch_size, 16, 5, 5]\n",
        "\n",
        "        # Flattened size = 16 * 5 * 5 = 400 features per image\n",
        "        # Fully connected layers:\n",
        "        self.fc1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=10)  # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: [batch_size, 3, 32, 32]\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # → [batch_size, 6, 14, 14]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # → [batch_size, 16, 5, 5]\n",
        "        x = torch.flatten(x, 1)               # → [batch_size, 16*5*5] = [batch_size, 400]\n",
        "        x = F.relu(self.fc1(x))               # → [batch_size, 120]\n",
        "        x = F.relu(self.fc2(x))               # → [batch_size, 84]\n",
        "        x = self.fc3(x)                       # → [batch_size, 10] (class scores)\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)\n",
        "print(net.conv1)"
      ],
      "metadata": {
        "id": "YB99ezb3f1ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stare carefully at the comments in the code above - this is the yoga of CNN dimension bookkeeping. One other word we haven't mentioned yet - the **stride** of the filters is by default `1`, which means they'll slide pixel-by-pixel across the image. However, that's not the only possibility!\n",
        "\n",
        "Here's a visual of what `stride=2` might look like:\n",
        "\n",
        "![stride2](https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_arithmetic_-_No_padding_strides.gif \"stride2\")\n",
        "\n",
        "To put it carefully, in this image we're applying a 3x3 operation (either pooling or convolution) with stride 2."
      ],
      "metadata": {
        "id": "sRQxdZX3zoGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "(3 mins)  Knowing this, what is the shape of an image after the pooling stage? Can you make sense of the shapes provided to initialize `self.conv2`? What about the output of `self.conv2`?\n"
      ],
      "metadata": {
        "id": "jYaBCHEywWzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = images[0:1].to(device)\n",
        "print(\"Initial image shape:\", image.shape)\n",
        "image_conv1 = net.conv1(image)\n",
        "print(\"After conv1:\", image_conv1.shape)\n",
        "\n",
        "# TO-DO: pass image_conv1 to the pooling operator\n",
        "\n",
        "# TO-DO: pass image_pool to the conv2 operator\n"
      ],
      "metadata": {
        "id": "Kdv_fI7qyQ8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a classification problem, we're going to optimize a cross entropy loss. Our strategy will be simple stochastic gradient descent."
      ],
      "metadata": {
        "id": "OmeVuPMBoxGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "1OL8WbC0apGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's our training loop - it's fairly standard, so we won't comment too much. Let's press play and train the network!"
      ],
      "metadata": {
        "id": "21HB5nQlo1zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "PP5Pspsto0Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(testloader)\n",
        "test_data = next(dataiter)\n",
        "images, labels = test_data[0].to(device), test_data[1].to(device)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n"
      ],
      "metadata": {
        "id": "XspTScH6o4p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))"
      ],
      "metadata": {
        "id": "CO1hHBbS4zAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "voSXyQGX49Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "mqbz3dEx8ucF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 3**\n",
        "\n",
        "Can you improve these accuracies? Full notebook's worth of code reproduced below for convenience.\n"
      ],
      "metadata": {
        "id": "djn_HaxgEsQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" # or \"cuda\" if you want to use the GPU!\n",
        "\n",
        "classes = (\n",
        "    'plane', 'car', 'bird', 'cat', 'deer',\n",
        "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
        ")\n",
        "\n",
        "# transform RGB features from [0,1]x3 to [-1,1]x3\n",
        "transform = transforms.Compose(\n",
        "  [ # image = (image - mean) / std\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(\n",
        "          (0.5, 0.5, 0.5), # means\n",
        "          (0.5, 0.5, 0.5) # stds\n",
        "      )\n",
        "   ]\n",
        ")\n",
        "\n",
        "# grab the train and test sets\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        "  )\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# feed the train/test sets into data loaders\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5, 1) # in_channels, out_channels, kernel_size, stride\n",
        "        self.pool = nn.MaxPool2d(2, 2) # kernel_size, stride\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, 1) # in_channels, out_channels, kernel_size, stride\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # in_channels, out_channels\n",
        "        self.fc2 = nn.Linear(120, 84) # in_channels, out_channels\n",
        "        self.fc3 = nn.Linear(84, 10) # in_channels, out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x))) # learnable convolution, pool result\n",
        "        x = self.pool(F.relu(self.conv2(x))) # learnable convolution, pool result\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x)) # apply a fully-connected NN\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x # return classification scores\n",
        "\n",
        "net = Net().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "id": "sthwvOOfFBd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solutions to Exercises\n"
      ],
      "metadata": {
        "id": "v8Mbx1aAUTvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "#Exercise 1a\n",
        "\n",
        "# returns [4, 3, 32, 32], corresponding to [batch_size, RGB, ix, iy]\n",
        "images.shape\n",
        "```"
      ],
      "metadata": {
        "id": "jXVoKwbtUEwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Exercise 1b\n",
        "\n",
        "# plot the \"R\" features only as greyscale\n",
        "imshow(torchvision.utils.make_grid(images[:,0:1,:,:]))\n",
        "```"
      ],
      "metadata": {
        "id": "jyd9WdGiUPBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Exercise 2\n",
        "\n",
        "# 2. (3 mins)  Knowing this, what is the shape of an image after the pooling stage? Can you make sense of the shapes provided to initialize `self.conv2`? What about the output of `self.conv2`?\n",
        "\n",
        "image = images[0:1].to(device)\n",
        "print(\"Initial image shape:\", image.shape)\n",
        "image_conv1 = net.conv1(image)\n",
        "print(\"After conv1:\", image_conv1.shape)\n",
        "image_pool = net.pool(image_conv1)\n",
        "print(\"After pooling:\", image_pool.shape)\n",
        "image_conv2 = net.conv2(image_pool)\n",
        "print(\"After conv2:\", image_conv2.shape)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Ch48wZVrm0s6"
      }
    }
  ]
}