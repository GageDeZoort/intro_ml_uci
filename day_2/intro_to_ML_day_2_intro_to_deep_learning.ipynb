{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RbxD4BGybG9"
      },
      "source": [
        "# UCI Introduction to Machine Learning\n",
        "**Day 2: Intro to Deep Learning**\n",
        "\n",
        "\n",
        "Notebook adapted by Gage DeZoort from a similar notebook offered in Princeton University's [Introduction to Machine Learning Wintersession course](https://github.com/PrincetonUniversity/intro_machine_learning/tree/main).\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day3/Intro_Machine_Learning_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuOQULmty2c0"
      },
      "source": [
        "# 0. Separating Moons via Logistic Regression\n",
        "\n",
        "Let's start with a binary classification problem that is difficult to solve with a linear classification boundary - that is, you can't draw a line to separate the classes. This exercise is adapted from [Denny Britz](https://github.com/dennybritz/nn-from-scratch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igw_471E1ECa"
      },
      "source": [
        "## 0.1 Generating the Data\n",
        " To do this, we'll use the `scikit-learn` [`make_moons`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html) function to generate our dataset. In a nutshell, this generates noisey samples from two overlapping half-circles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcUSAmikzu24"
      },
      "outputs": [],
      "source": [
        "# package imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "import matplotlib\n",
        "\n",
        "import io\n",
        "import gc\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "# display plots inline and change default figure size\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHMiUQhx12h7"
      },
      "outputs": [],
      "source": [
        "## QUESTION: what happens as you vary noise from 0.2?\n",
        "X, y = sklearn.datasets.make_moons(n_samples=200, noise=0.2)\n",
        "\n",
        "# print some dataset properties\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"Classes: {np.unique(y)}\")\n",
        "\n",
        "# plot the data\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, dpi=100, figsize=(8,4))\n",
        "\n",
        "# first plot the unlabeled data (inputs to the model)\n",
        "axs[0].scatter(X[:,0], X[:,1], s=10, color=\"Black\", label=\"Input Data\")\n",
        "axs[0].set_xlabel(r\"$x_1$\")\n",
        "axs[0].set_ylabel(r\"$x_2$\")\n",
        "axs[0].set_title(\"Model Inputs\")\n",
        "\n",
        "# next plot the labeled data (which we'll use to train the model)\n",
        "axs[1].scatter(X[:,0][y==1], X[:,1][y==1], s=10, c=\"red\", label=\"Class 1\")\n",
        "axs[1].scatter(X[:,0][y==0], X[:,1][y==0], s=10, c=\"blue\", label=\"Class 0\")\n",
        "axs[1].legend(loc=\"best\")\n",
        "axs[1].set_xlabel(r\"$x_1$\")\n",
        "axs[1].set_ylabel(r\"$x_2$\")\n",
        "axs[1].set_title(\"Truth Labels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcYbkrLh3ZxM"
      },
      "source": [
        "We've generated a dataset in which two input features $(x_1, x_2)$ are associated to one of two class labels, 0 or 1.\n",
        "\n",
        "These class labels are drawn as red and blue above. Since this is a toy problem, we can come up with a fun interpretation of the data. Let's say each point represents a Netflix show that you'll either like (blue) or dislike (red).\n",
        "\n",
        "In this case, $x_1$ and $x_2$ would measure something about each show - quantities like the average user rating or the ratio of dialogue to action. Given $(x_1, x_2)$, how can we predict whether or not you'll like the show?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFVECyEF5tcr"
      },
      "source": [
        "## 0.2 Linear Classifier\n",
        "By eye, it looks pretty difficult to separate the data with a linear decision boundary. Let's see what happens when we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By1N53Gs5qd0"
      },
      "outputs": [],
      "source": [
        "# train a logistic regression classifier\n",
        "clf = sklearn.linear_model.LogisticRegressionCV()\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8X-_brg6Gsx"
      },
      "outputs": [],
      "source": [
        "# helper function to plot the output of the classifier\n",
        "def plot_decision_boundary(pred_func):\n",
        "    # set min and max values and give it some padding\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    h = 0.01\n",
        "    # generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # predict the function value for the whole grid\n",
        "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # plot the contour and training examples\n",
        "    fig = plt.figure(dpi=100, figsize=(4,4))\n",
        "    plt.title(\"Logistic Regression\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=plt.cm.coolwarm)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ1QNUvw6Zm3"
      },
      "outputs": [],
      "source": [
        "# plot the decision boundary\n",
        "plot_decision_boundary(lambda x: clf.predict(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5CstYat6dlE"
      },
      "outputs": [],
      "source": [
        "# print the accuracy of the classifier\n",
        "clf.score(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDLzX_NP7t9Q"
      },
      "source": [
        "By eye, the model did something reasonable - however, the linear decision boundary is clearly not appropriate for this problem. The training accuracy (at original noise level `.2`) should be around 85-87%, meaning roughly 13-15 examples from each class are mis-classified. The reason should be clear -- the data is not linearly separable. So can we find a model capable of fitting non-linear trends?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXRy6Jd8JwL"
      },
      "source": [
        "# 1. Neural Networks\n",
        "We can indeed! Let's implement a neural network (NN). We're going to step through this slowly, starting by implementing a NN by hand. We'll then turn to some modern deep learning libraries like `PyTorch`, `TensorFlow`, and `Keras` and tackle some harder problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHtCtLoG9BEW"
      },
      "source": [
        "## 1.1 Neural Networks by Hand\n",
        "### Overview\n",
        "Let's start by being precise about the ML task at hand. Our \"Netflix\" dataset associated 2 input features with a binary classification label:\n",
        "\n",
        "**Inputs**: $\\mathbf{x}_i= \\begin{bmatrix} x_{i,1} \\\\ x_{i,2} \\end{bmatrix}$\n",
        "\n",
        "**Class Labels**: $y_i\\in\\{0,1\\}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cggje_CeK0w3"
      },
      "source": [
        "When we use **integer labels** (e.g. 0, 1, 2) to represent classes, the model’s output lives on a one-dimensional line.\n",
        "This can make it seem like the classes have an ordering or magnitude — as if class “2” were somehow greater than class “1.”\n",
        "\n",
        "In contrast, **one-hot encodings** expands the target space: each class gets its own dimension, and the model learns to assign probabilities across all classes.  \n",
        "For example, for our two class classification problem, the one-hot encodings are\n",
        "\n",
        "$$\n",
        "\\text{Class 0} \\to [1, 0], \\qquad\n",
        "\\text{Class 1} \\to [0, 1].\n",
        "$$\n",
        "\n",
        "\n",
        "When trained on one-hot encodings, the model may output quantities like\n",
        "\n",
        "\n",
        "$$\n",
        "\\widehat{y_i} = [0.7, 0.3],\n",
        "$$\n",
        "\n",
        "representing an overall preference for Class 0 on the $i^\\mathrm{th}$ example, but not a clear preference. In this sense, one-hot encodings give the classifier more wiggle room.\n",
        "\n",
        "Remember:\n",
        "$$\\begin{align}\n",
        "  &\\text{integer encoding:} \\,0 \\rightarrow \\text{one-hot encoding:} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\\\\n",
        "  &\\text{integer encoding:} \\,1\\rightarrow \\text{one-hot encoding:} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzF44eRHB8t1"
      },
      "source": [
        "### NN Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqhE5JO8--UC"
      },
      "source": [
        "Here's the NN we're going to build:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://raw.githubusercontent.com/GageDeZoort/intro_ml_uci/main/day_2/images/day_2-nn_architecture.png\" width=\"300\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkk4XjFh_WDA"
      },
      "source": [
        "We have **2 input dimensions**, **1 hidden layer** containing **5 neurons**, and **2 output dimensions**.\n",
        "\n",
        "We can increase the size of the model by ramping up the number of neurons in the hidden layer; this number is called the **width** of the model. Intuitively, models with larger width are more **expressive**, i.e. capable of modeling a wider class of functions.\n",
        "\n",
        "However, we have to watch out for **overfitting**, wherein a model is large enough that it can \"memorize\" the training data while failing to generalize to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Remember, forward propagation is a series of matrix additions and multiplications, so it's important to get the dimensions right. Let's drill down a bit more. Consider sets of _weights_ and _biases_ with the following dimensions:\n",
        "\n",
        "\n",
        "$$ \\mathbf{W}_1\\in\\mathbb{R}^{n_\\mathrm{hidden}\\,\\times\\, 2} \\qquad \\mathbf{b}_1\\in\\mathbb{R}^{n_\\mathrm{hidden}}$$\n",
        "\n",
        "$$ \\mathbf{W}_2\\in\\mathbb{R}^{2\\,\\times\\, n_\\mathrm{hidden}}\\qquad \\mathbf{b}_2\\in\\mathbb{R}^{2}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "coOprTNTySp4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUSN0g85C8Vt"
      },
      "source": [
        "The dimension of the hidden layer is $n_\\mathrm{hidden}$; this was 5 in the diagram above, but here we've generalized it.\n",
        "The weights and biases are very important; these are the **trainable parameters** in your network, i.e. the numbers being adjusted via gradient descent training.\n",
        "The **forward pass** of the neural network refers to the calculations used to compute an output, given an input:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "  \\mathrm{Input}:\\ & \\mathbf{x}_i\\in\\mathbb{R}^2 \\\\\n",
        "  \\mathrm{First\\ Preactivations}:\\ & \\mathbf{z}_1 = \\mathbf{W}_1 \\mathbf{x}_i + \\mathbf{b}_1\\\\\n",
        "  \\mathrm{First\\ Postactivations}:\\ & \\sigma( \\mathbf{z}_1)\\\\\n",
        "  \\mathrm{Second\\ Preactivations}:\\ & \\mathbf{z}_2 = \\mathbf{W}_2\\sigma(\\mathbf{z}_1) + \\mathbf{b}_2\\\\\n",
        "  \\mathrm{Output}:\\ & \\widehat{\\mathbf{y}}_i = \\texttt{Softmax}(\\mathbf{z}_2)\\in\\mathbb{R}^2\n",
        "  \\end{align}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwOXo2I8I13t"
      },
      "source": [
        "Here, $\\sigma(\\cdot)$ is a non-linear activation function - more on this in a moment.\n",
        "\n",
        "First, check that the dimensions work out in our NN above! Here's a helpful exercise:\n",
        "\n",
        "> **EXERCISE 1**: If we take $n_\\mathrm{hidden}=500$, how many trainable parameters does the network have?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 1"
      ],
      "metadata": {
        "id": "Dpfb__N81le7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the neural network above, we've used an **activation function** $\\sigma(\\cdot)$, which is commonly called a **nonlinearity** because it induces a nonlinear connection between the trainable weights in the network. We can see this by writing the forward pass in one line:\n",
        "\n",
        "$$\n",
        "\\widehat{y}_i = \\texttt{Softmax}(\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{x}_i + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "There are many common activation functions.\n",
        "\n",
        "- **Sigmoid**:\n",
        "  \n",
        "  (+) Smooth; outputs in (0,1) interpret as \"on-off\" switch\n",
        "\n",
        "  (–) Non–zero-centered; strong saturation for |x|>>0 -> vanishing gradients\n",
        "\n",
        "  (–) Gradients max at x≈0 (σ'(0)=0.25) but decay quickly\n",
        "\n",
        "  (-) Slow to compute due to dependence on exp\n",
        "\n",
        "- **tanh**:\n",
        "\n",
        "  (+) Zero-centered outputs in (-1,1)\n",
        "\n",
        "  (+) Larger active gradient region around 0 (tanh'(0)=1)\n",
        "\n",
        "  (–) Still saturates for large |x| -> vanishing gradients at the tails\n",
        "\n",
        "  (-) Slow to compute due to dependence on exp\n",
        "\n",
        "- **ReLU**:\n",
        "\n",
        "  (+) Simple & fast; no saturation for x>0\n",
        "\n",
        "  (+) Mitigates vanishing gradients in positive region (derivative = 1)\n",
        "\n",
        "  (–) “Dying ReLU” (units stuck at 0 when inputs stay negative)"
      ],
      "metadata": {
        "id": "7V5rK8vJ2gkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "\n",
        "# activation functions\n",
        "sig = 1 / (1 + np.exp(-x))\n",
        "tanh = np.tanh(x)\n",
        "relu = np.maximum(0, x)\n",
        "\n",
        "# corresponding derivatives\n",
        "dsig = sig * (1 - sig)              # sigmoid'(x)\n",
        "dtanh = 1 - tanh**2                 # tanh'(x)\n",
        "drelu = (x > 0).astype(float)       # ReLU'(x); undefined at 0 -> set to 0\n",
        "\n",
        "fig, axs = plt.subplots(2, 3, figsize=(12, 6), dpi=120, sharex='col', sharey=\"row\")\n",
        "\n",
        "# plot the functions\n",
        "axs[0, 0].plot(x, sig, label='sigmoid')\n",
        "axs[0, 0].set_title(\"Sigmoid\")\n",
        "axs[0, 0].set_ylabel(\"σ(z)\")\n",
        "axs[0, 1].plot(x, tanh, label='tanh',)\n",
        "axs[0, 1].set_title(\"tanh\")\n",
        "axs[0, 2].plot(x, relu, label='ReLU')\n",
        "axs[0, 2].set_title(\"ReLU\")\n",
        "\n",
        "# plot the derivatives\n",
        "axs[1, 0].plot(x, dsig, label=\"σ'(z)\")\n",
        "axs[1, 0].set_ylabel(\"Derivative\")\n",
        "axs[1, 1].plot(x, dtanh, label=\"tanh'(z)\")\n",
        "axs[1, 2].plot(x, drelu, label=\"ReLU'(z)\")\n",
        "\n",
        "axs[0,0].set_ylim([-2.5, 2.5])\n",
        "for ax in axs[1, :]:\n",
        "    ax.set_xlabel(\"z\")\n",
        "\n",
        "# Light grids & tight layout\n",
        "for ax in axs.ravel():\n",
        "    ax.grid(alpha=0.25)\n",
        "\n",
        "fig.suptitle(\"Common Activation Functions and Their Derivatives\", y=1.02, fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-nUKcdpU3gcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47t6vomJI1qa"
      },
      "source": [
        "You may also have noticed that our outputs depend on a function called **softmax**.\n",
        "\n",
        "The softmax function normalizes the two raw components of $\\mathbf{z}_2$ so that they indicate the likelihood of $\\mathbf{x}_i$ being either blue or red.\n",
        "\n",
        "For reference, if you have $C$ truth classes (here we have $C=2$), softmax returns a vector of size $C$ with components\n",
        "\n",
        "$$ \\texttt{Softmax}(\\mathbf{z})_c = \\frac{e^{\\mathbf{z}_c}}{\\sum_{c=1}^C e^{\\mathbf{z}_c}}\\in\\mathbb{R}^C$$\n",
        "\n",
        "In our case, because we have $C=2$,\n",
        "\n",
        "$$ Softmax([z_1, z_2]) = \\bigg[\\frac{e^{z_1}}{e^{z_1} + e^{z_2}},\\ \\ \\frac{e^{z_2}}{e^{z_1} + e^{z_2}}\\bigg].$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 2**: What's the difference between standard normalization and softmax normalization? Try normalizing the vectors [0.5, 1] and [5, 10] with both methods."
      ],
      "metadata": {
        "id": "3buVTzsjJq72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 2"
      ],
      "metadata": {
        "id": "d2I6yQMNyXZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqHWTqiMO8yZ"
      },
      "source": [
        "**Techincal Note:** `ReLU`, `tanh` and `sigmoid` are each functions of a single variable. When used as $\\sigma$ and applied to vectors, they act on each coordinate of the vector, applying the same single-variable function to each coordinate. That is,\n",
        "\n",
        "$$ \\text{ReLU}(\\mathbf{z}) = \\begin{bmatrix}\\text{ReLU}(z_1)\\\\ \\text{ReLU}(z_2)\\end{bmatrix}$$\n",
        "\n",
        "In contrast, `Softmax` scales every component of $\\mathbf{z}$ in a way that depends on every other component! This is because of the sum in the denominator, the normalization factor $\\sum_c e^{z_c}$. These positive entries summing to one can be interpreted as _probabilities_. More specifically, they describe a full _probability distribution_ over the output coordinates.\n",
        "\n",
        "Changing one cooordinate of $\\mathbf{z}$ will change all coordinates of $\\texttt{Softmax}(\\mathbf{z})$. However: you can check that shifting all coordinates of $\\mathbf{x}$ by the _same_ number will _not_ change its softmax."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8KuME199Wn5"
      },
      "source": [
        "Okay, time to code up this NN architecture.\n",
        "\n",
        "We're going to choose $\\sigma(\\mathbf{z})=\\tanh(\\mathbf{z})$ in this case because its derivative, $1-\\tanh^2(\\mathbf{z})$, is something we can calculate immediately from the value of $\\tanh(\\mathbf{z})$. By the way, this function can be written in terms of exponentials:\n",
        "\n",
        "$$\n",
        "\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^z + e^{-z}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Yp2DQCfDFb"
      },
      "source": [
        "In building our NN, we have to supply initial values for $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$. This process is called **initialization** and is important to starting with a trainable NN. More on this after the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEUsvaMY45Lc"
      },
      "outputs": [],
      "source": [
        "# we sampled 200 points\n",
        "n_examples = len(X)\n",
        "\n",
        "# each of the 200 points has 2 features (x1, x2)\n",
        "input_dim = X.shape[-1]\n",
        "\n",
        "# we want to predict class probabilities for 2 classes\n",
        "output_dim = len(np.unique(y))\n",
        "\n",
        "n_examples, input_dim, output_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6RQzrZF6f8g"
      },
      "outputs": [],
      "source": [
        "# now let's build the  NN!\n",
        "\n",
        "class Model():\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # assign some learnable parameters to the NN (random normal variables)\n",
        "    # W1 is a 2 x n_hidden matrix\n",
        "    self.W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
        "    # b1 is a 1 x n_hidden matrix\n",
        "    self.b1 = np.zeros((1, hidden_dim))\n",
        "    # W2 is a n_hidden x 2 matrix\n",
        "    self.W2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
        "    # b2 is a 1 x 2 matrix\n",
        "    self.b2 = np.zeros((1, output_dim))\n",
        "\n",
        "  # given X with dimension 200x2, predict a \\hat{y} with dimension 200x2\n",
        "  def forward_pass(self, X):\n",
        "    self.z1 = X @ self.W1 + self.b1 # @ is a shorthand for matrix multiplication\n",
        "    self.h1 = np.tanh(self.z1)\n",
        "    self.z2 = self.h1 @ self.W2 + self.b2\n",
        "    self.output = self.softmax(self.z2)\n",
        "    return self.output\n",
        "\n",
        "  def softmax(self, z):\n",
        "    exp_scores = np.exp(z)\n",
        "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "    return probs\n",
        "\n",
        "# build a test model with a hidden layer of size 5\n",
        "model = Model(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=5,\n",
        "    output_dim=output_dim,\n",
        ")\n",
        "\n",
        "# let's see what the network (randomly initialized) outputs for the first 5 examples\n",
        "print(X.shape)\n",
        "model.forward_pass(X)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoDTUbH6fS-V"
      },
      "source": [
        "Note how we initialize the weights $\\mathbf{W}_1$ in terms of the input size and $\\mathbf{W}_2$ in terms of hidden layer size. The optimal way to do this will depend on the activation function being used; the heuristics behind these choices aim at preserving the variance of the input signal as it propagates through the network.\n",
        "\n",
        "As the following cartoon examples suggest, this becomes more important as the number of hidden layers (network depth) increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts2ReSIOSkDr"
      },
      "outputs": [],
      "source": [
        "# Dumb neural network: Ax, then AAx, then AAAx, then..., then (A)^n x\n",
        "\n",
        "# cartoon: suppose that the scale of the weights is 1.5 x\n",
        "# what it \"should\" be, at each layer:\n",
        "A = np.array([[1.5, 0], [0, 1.5]])\n",
        "print(A,\"\\n\\n\", A @ A, \"\\n\\n\", np.linalg.matrix_power(A, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values are exponentially exploding! Let's look at another scenario:"
      ],
      "metadata": {
        "id": "MtB3LDCjNGD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bMB2qWQTVqD"
      },
      "outputs": [],
      "source": [
        "# Dumb neural network: Bx, then BBx, then BBBx, then..., then (B)^n x\n",
        "\n",
        "# or suppose that the scale of the weights is .75 x\n",
        "# what it \"should\" be, at each layer:\n",
        "B = np.array([[.75, 0], [0, .75]])\n",
        "print(B,\"\\n\\n\", B @ B, \"\\n\\n\", np.linalg.matrix_power(B, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exponentially vanishing...\n"
      ],
      "metadata": {
        "id": "AT7ZqRXkNQOH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bnPvsWF8hPj"
      },
      "source": [
        "## 1.2 Training the NN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45eYclZCdjtb"
      },
      "source": [
        "### Loss Function\n",
        "All that remains is to train the NN - i.e. tune its learnable parameters - to separate our data points.\n",
        "\n",
        "Our strategy will be to define a *loss function* that the NN will minimize. In doing so, the NN will learn to perform the classification task. Since the output of the NN is fed through `Softmax`, we can interpret the 2D outputs $\\widehat{\\mathbf{y}}_i$ as probabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KziGXXlpjEiJ"
      },
      "source": [
        "We'll be using the *cross entropy* loss function, which is designed to penalize the network in proportion to how \"wrong\" it was. For a single input $\\mathbf{x}_i$ with truth label $\\mathbf{y}_i$, and a NN's prediction $\\widehat{\\mathbf{y}}_i$, the cross entropy loss of the prediction is defined as a sum over the number ($C$) of truth classes:\n",
        "\n",
        "$$l(\\mathbf{y}_i, \\widehat{\\mathbf{y}}_i) = -\\sum_{c=1}^C \\mathbf{y}_{i,c}\\log\\widehat{\\mathbf{y}}_{i,c}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cbrBWvvkWnI"
      },
      "source": [
        "Remember, we're using one-hot encodings, so $\\mathbf{y}_i$ and $\\widehat{\\mathbf{y}}_i$ are vectors of size $C$. In training the network, we will make a prediction on every input $x_i$, so let's define the total loss as the average over the sample (with arbitrary size $N$):\n",
        "\n",
        "$$L(\\mathbf{y},\\widehat{\\mathbf{y}}) = -\\frac{1}{N}\\sum_{i=1}^N l( \\mathbf{y}_i,\\widehat{\\mathbf{y}}_i) = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{c=1}^C \\mathbf{y}_{i,c}\\log\\widehat{\\mathbf{y}}_{i,c}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **EXERCISE 3**: Yesterday, we looked at binary cross entropy (BCE), which operated on a single sigmoid-normalized input. How does BCE relate to this multi-class cross entropy, where we now use softmax-normalized inputs?"
      ],
      "metadata": {
        "id": "wnapFBSfOZI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 3"
      ],
      "metadata": {
        "id": "H7JUXDxxOwoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1DSrgQnS327"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y_true, y_pred):\n",
        "    logprobs = np.log(y_pred)\n",
        "    sums = (y_true * logprobs).sum()\n",
        "    return -sums/len(y_true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THgki52mjNUF"
      },
      "source": [
        "**Technical Note:** _Entropy_ measures the degree of unpredictability of a random variable. Suppose we have a set $\\{ \\alpha_1, \\dots, \\alpha_C\\}$ of categories, and some assignment of probabilities $p_c$ to each category $\\alpha_c$. These probabilities are collected in a vector\n",
        "$\\mathbf{p} = (p_1, \\dots, p_C)$.\n",
        "\n",
        "We define the **surprise** of $\\alpha_c$ according to $\\mathbf{p}$, to be $-\\log p_c$. The negative sign appears because $0\\leq p_c\\leq 1$, so $\\log p_c \\leq 0$. The more rare the symbol (according to $\\mathbf{p}$), the larger its surprise. If we have any assignment of values to symbols, $f : \\{ \\alpha_1, \\dots, \\alpha_C\\} \\to \\mathbb{R}$, we can compute its average (or \"expected\") value with respect to $\\mathbf{p}$ as:\n",
        "$$\n",
        "E_{\\mathbf{p}}f = \\sum_{c=1}^C p_c f(\\alpha_c)\n",
        "$$\n",
        "In the special case of this formula, where we compute the average value  (according to $\\mathbf{p}$) of $\\mathbf{p}$'s surprise function, we recover the formula for the entropy of $\\mathbf{p}$:\n",
        "$$\n",
        "S( \\mathbf{p}) = - \\sum_{c=1}^C p_c \\log p_c\n",
        "$$\n",
        "\n",
        "$\\vdots$\n",
        "\n",
        "So what is the interpretation of _cross-entropy_? It is what we get if we replace the surprise function according to $\\mathbf{p}$ with the surprise function of another probability vector $\\mathbf{q} = (q_1, \\dots, q_n)$, while still computing the average with respect to $\\mathbf{p}$:\n",
        "$$\n",
        "L(\\mathbf{p}, \\mathbf{q}) = -\\sum_{c=1}^C  p_c \\log q_c\n",
        "$$\n",
        "Importantly, the smallest $L(\\mathbf{p}, \\mathbf{q})$ can be is $S(\\mathbf{p})$. The cross-entropy quantifies the additional surprise we incur by assuming our data comes from distribution $\\mathbf{q}$, when the true distribution is $\\mathbf{p}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdRMC-KlHPxl"
      },
      "source": [
        "### Gradient Descent\n",
        "To train the NN, we need to minimize the cross entropy function. We're going to use gradient descent to try and find the minimum.\n",
        "\n",
        "The logic goes something like this: $L$ is a function of the NN's prediction $\\widehat{\\mathbf{y}}$. In turn, $\\widehat{\\mathbf{y}}$ was calculated using the learnable parameters in $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$. We can make adjustments to $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, and $\\mathbf{b}_2$ so that the loss function decreases:\n",
        "\n",
        "$$\\begin{align}\n",
        "  \\mathbf{W}_1 &\\leftarrow \\mathbf{W}_1 - \\eta\\frac{\\partial L}{\\partial \\mathbf{W}_1}\\\\\n",
        "  \\mathbf{b}_1 &\\leftarrow \\mathbf{b}_1 - \\eta\\frac{\\partial L}{\\partial \\mathbf{b}_1}\\\\\n",
        "  \\mathbf{W}_2 &\\leftarrow \\mathbf{W}_2 - \\eta\\frac{\\partial L}{\\partial \\mathbf{W}_2}\\\\\n",
        "  \\mathbf{b}_2 &\\leftarrow \\mathbf{b}_2 - \\eta\\frac{\\partial L}{\\partial \\mathbf{b}_2}\n",
        "\\end{align}$$\n",
        "\n",
        "Here $\\eta$ is a **hyperparameter** (a parameter that is *not* updated by gradient descent, but that nonetheless must be adjusted to properly train the model) called the **learning rate**.\n",
        "\n",
        "Note, the relevant derivatives are shown in the cell below - don't worry too much about them right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFPKXSwDDnjp"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model, X, y_true, n_examples, n_epochs=20000,\n",
        "    log_results=False, learning_rate=0.01,\n",
        "    plot_training=False\n",
        "):\n",
        "\n",
        "    accs, losses = [], []\n",
        "\n",
        "    # epoch = train over the whole dataset one full time\n",
        "    for i in range(n_epochs):\n",
        "        # calculate the loss terms\n",
        "        y_pred = model.forward_pass(X)\n",
        "        delta3 = y_pred - y_true\n",
        "        \"\"\"\n",
        "        This looks like the gradient for the MSE loss,\n",
        "        but we are actually doing two steps at once here:\n",
        "        we are taking the derivative of the cross entropy,\n",
        "        not with respect to the output of the softmax, but rather its input.\n",
        "        When one computes the derivative of cross-entropy with respect to\n",
        "        these logits (input to softmax), there is nice cancellation,\n",
        "        and one obtains the above formula.\n",
        "        \"\"\"\n",
        "        dW2 = (model.h1.T).dot(delta3)\n",
        "        \"\"\"\n",
        "        Since h1 is a constant with respect to W2 (it's the output from\n",
        "        the previous layer and does not change when W2 changes), the\n",
        "        derivative of y_pred with respect to W2 is just h1\n",
        "        \"\"\"\n",
        "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
        "        \"\"\"\n",
        "        we compute delta2 with our specific knowledge of\n",
        "        the derivative of tanh\n",
        "        \"\"\"\n",
        "        delta2 = delta3.dot(model.W2.T) * (1 - np.power(model.h1, 2))\n",
        "        dW1 = np.dot(X.T, delta2)\n",
        "        db1 = np.sum(delta2, axis=0)\n",
        "\n",
        "        # Gradient descent parameter update\n",
        "        model.W1 += -learning_rate * dW1\n",
        "        model.b1 += -learning_rate * db1\n",
        "        model.W2 += -learning_rate * dW2\n",
        "        model.b2 += -learning_rate * db2\n",
        "\n",
        "        n_correct=(np.argmax(y_pred, axis=1)==np.argmax(y_true, axis=1)).sum()\n",
        "        acc = n_correct/n_examples\n",
        "        accs.append(acc)\n",
        "        loss = cross_entropy(y_true, y_pred)\n",
        "        losses.append(loss)\n",
        "        if log_results and i % 25 == 0:\n",
        "          print(f\"Epoch {i}: Loss={loss:.4f}, Acc={acc:.4f}\")\n",
        "\n",
        "    # print results\n",
        "    if plot_training:\n",
        "        fig, axs = plt.subplots(ncols=2, dpi=100, figsize=(8, 4))\n",
        "        axs[0].plot(accs)\n",
        "        axs[0].set_xlabel(\"Epoch\")\n",
        "        axs[0].set_ylabel(\"Train Accuracy\")\n",
        "        axs[1].plot(losses)\n",
        "        axs[1].set_xlabel(\"Epoch\")\n",
        "        axs[1].set_ylabel(\"Train Loss\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "# randomly initialize a model\n",
        "model = Model(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=10,\n",
        "    output_dim=output_dim,\n",
        ")\n",
        "\n",
        "# train the model\n",
        "y_true = np.eye(2)[y] # reshape y to match 2D network outputs (called a one-hot encoding)\n",
        "model = train_model(\n",
        "    model, X, y_true, n_examples, n_epochs=400,\n",
        "    log_results=False, plot_training=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each training **epoch**, this model trains on the _entire set of training data_.\n",
        "\n",
        "The model parameters are updated only once per epoch, which can slow down training on larger datasets. Moreover, we may run into memory limitations, as the entire training dataset is loaded and processed at once."
      ],
      "metadata": {
        "id": "iAlKJlHbE8EE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqg6rHzgLLUw"
      },
      "source": [
        "Overall, the training results looks nice! It looks like the accuracy of the model steadily increased as the loss decreased. We've effectively \"taught\" this NN our dataset. Let's check the decision boundary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqtsULaLLXjQ"
      },
      "outputs": [],
      "source": [
        "plot_decision_boundary(lambda x: np.argmax(model.forward_pass(x), axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv_Vv3EK449w"
      },
      "source": [
        "Looks great! Pretty close to what you'd draw by hand, right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80rCxsrsVpUK"
      },
      "source": [
        "**EXERCISE 4**. Train multiple networks with a range of hidden dimension sizes and explore the effect this has on the decision boundary. Try plotting several decision boundaries side-by-side as the hidden dimension increases. In what sense does your model appear to be getting better/worse?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm8Je-s-5huS"
      },
      "outputs": [],
      "source": [
        "# animation code partially taken from the Part 2 notebook :)\n",
        "# helper function to plot the output of the classifier\n",
        "def plot_decision_boundary_alt(pred_func, ax, title):\n",
        "    # set min and max values and give it some padding\n",
        "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "    h = 0.01\n",
        "    # generate a grid of points with distance h between them\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    # predict the function value for the whole grid\n",
        "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    # plot the contour and training examples on the provided axes\n",
        "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.2)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(r\"$x_1$\")\n",
        "    ax.set_ylabel(r\"$x_2$\")\n",
        "    return [ax.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=plt.cm.coolwarm)]\n",
        "\n",
        "models = []\n",
        "for h in range(5, 100, 5):\n",
        "    model = Model(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=h,\n",
        "        output_dim=output_dim,\n",
        "    )\n",
        "    y_true = np.eye(2)[y]  # reshape y to match 2D network outputs\n",
        "    model = train_model(model, X, y_true, n_examples, 400, log_results=False)\n",
        "    models.append(model)\n",
        "\n",
        "test_model = models[0]  # Assuming models[0] is a trained model\n",
        "fig, ax = plt.subplots(dpi=100)\n",
        "# plot_decision_boundary_alt(lambda x: np.argmax(test_model.forward_pass(x), axis=1), ax)\n",
        "# plt.show()\n",
        "\n",
        "# Function to update the animation frame\n",
        "def update(frame):\n",
        "    ax.clear()  # Clear the previous plot\n",
        "    model = models[frame]\n",
        "    title = f\"Model with hidden layer size: {5 + 5 * frame}\"\n",
        "    return plot_decision_boundary_alt(lambda x: np.argmax(model.forward_pass(x), axis=1), ax, title)\n",
        "\n",
        "# Create animation with the updated function\n",
        "ani = FuncAnimation(fig, update, frames=len(models), interval=200, blit=True)\n",
        "plt.close()\n",
        "\n",
        "# Display the animation\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OpNtdP1EZlD"
      },
      "outputs": [],
      "source": [
        "# saving RAM\n",
        "del models, ani\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSEc85pVV1WK"
      },
      "source": [
        "**EXERCISE 5**. Train multiple networks with different learning rates and explore the effect on the decision boundary. Try training the networks with an exponentially decaying learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UgfM4amVni_"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "lr_range = np.arange(0.0, 0.1, 0.003)\n",
        "for lr in lr_range:\n",
        "    model = Model(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=10, # fix hidden layer width to 10\n",
        "        output_dim=output_dim,\n",
        "    )\n",
        "    y_true = np.eye(2)[y]  # reshape y to match 2D network outputs\n",
        "    model = train_model(model, X, y_true, n_examples, 400,\n",
        "                        log_results=False, learning_rate=lr)\n",
        "    models.append(model)\n",
        "\n",
        "test_model = models[0]  # Assuming models[0] is a trained model\n",
        "fig, ax = plt.subplots()\n",
        "# plot_decision_boundary_alt(lambda x: np.argmax(test_model.forward_pass(x), axis=1), ax)\n",
        "# plt.show()\n",
        "\n",
        "# Function to update the animation frame\n",
        "def update(frame):\n",
        "    ax.clear()  # Clear the previous plot\n",
        "    model = models[frame]\n",
        "    lr = lr_range[frame]  # Get the corresponding learning rate\n",
        "    title = f\"Model with learning rate: {lr:.3f}\"\n",
        "    return plot_decision_boundary_alt(lambda x: np.argmax(model.forward_pass(x), axis=1), ax, title)\n",
        "\n",
        "# Create animation with the updated function\n",
        "ani = FuncAnimation(fig, update, frames=len(models), interval=200, blit=True)\n",
        "plt.close()\n",
        "\n",
        "# Display the animation\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdViXlWfFUpR"
      },
      "outputs": [],
      "source": [
        "# saving RAM\n",
        "del models, ani\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPhFRtDFLscd"
      },
      "source": [
        "# 2. Neural Networks in PyTorch\n",
        "\n",
        "Phew, that was a lot of work. Let's train some NNs the easy way - using a modern deep learning (DL) library called PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sorhc1LdR8g"
      },
      "source": [
        "## 2.1 Reading Handwritten Digits\n",
        "\n",
        "As a working example, let's take a look at the MNIST handwritten digits dataset. Each sample is an image $\\mathbf{x}_i$ and a truth label $y_i = 0, 1, \\dots, 9$, corresponding to the number in the image.\n",
        "\n",
        "Some of the below code is adapted from the PyTorch Docs, e.g. the [Datasets and Dataloaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGMgIxA6c5n4"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDYhsIaahaVI"
      },
      "source": [
        "### 2.1.0 Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhb0iqPDekoE"
      },
      "outputs": [],
      "source": [
        "# grab the data and normalize it\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmyN-gCr1xLE"
      },
      "source": [
        "Nice, we've got some data - in the following cell we'll explore it a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6xBJmhlJowg"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZvoRa6_JtG4"
      },
      "outputs": [],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdFk-O8Afkdb"
      },
      "outputs": [],
      "source": [
        "# print some dataset properties\n",
        "x0, y0 = train_data[0]\n",
        "print(f\"First image shape: {x0.shape}\")\n",
        "print(f\"First image label: {y0}\")\n",
        "\n",
        "# plot the first image\n",
        "fig = plt.figure(dpi=100, figsize=(4,4))\n",
        "plt.imshow(x0.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzwy3g-W149v"
      },
      "source": [
        "We see that each input is an image with $28\\times 28$ pixels, i.e. $\\mathbf{x}_i\\in\\mathbb{R}^{28\\times 28}$.\n",
        "\n",
        "Each of the pixels has a \"brightness\" value associated with it. This is because it represents a digtized image of a handwritten digit - each pixel's \"brightness\" corresponds to the presence of ink on the page.\n",
        "\n",
        "The truth labels just show you what number is in the image, in this case $y_0=5$. There are 60,000 train images and 10,000 test images. Let's plot a few more examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlNPeNnygLUi"
      },
      "outputs": [],
      "source": [
        "# plot several random examples\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n",
        "    img, label = train_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNIqf0RHhgSx"
      },
      "source": [
        "### 2.1.1 Data Preparation\n",
        "\n",
        "Now that we've taken some time to look at the data, let's use the default `MNIST DataLoaders` provided by `PyTorch`.\n",
        "\n",
        "We'll apply several transformations to the images.\n",
        "\n",
        "First, we'll ensure that they're stored as torch `tensor` objects, which you can think of as high-dimensional matrices that have gradients attached to them.\n",
        "\n",
        "Next, we'll normalize the data to have mean $0$ and standard deviation $1$ using the constants provided by `PyTorch`. Finally, we'll **flatten** the images into $28 \\times 28=784$ dimensional arrays so that we can pass them into the NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82aX3KiphETj"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBiAnYtEiegc"
      },
      "outputs": [],
      "source": [
        "# let's flatten and normalize the data\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "    transforms.Lambda(lambda x: torch.flatten(x))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXEnQdEh3NO4"
      },
      "source": [
        "Let's look at the first image in the train set and check that it is now a vector of size $784$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_aDeM-4k1GY"
      },
      "outputs": [],
      "source": [
        "# get the dimensions right - the original image was [1, 28, 28])\n",
        "x0, y0 = train_data[0]\n",
        "print(f\"Image Shape: {x0.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3wOGK0V3V0d"
      },
      "source": [
        "With our datasets in hand, `PyTorch` provides `DataLoader` classes designed to sample **batches** of data. Batching allows us to make predictions on several inputs at once before computing the gradients and using them to update the model. When we compute a gradient update on a randomly drawn batch of data and use it to update the model, this is called **stochastic gradient descent** (SGD) - stochastic because the batches have random composition.\n",
        "\n",
        "In this case, we pull a batch size of 128 images from the train set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ5TzBzAk7OI"
      },
      "outputs": [],
      "source": [
        "# now let's build some data loaders\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loader_mean_std(loader):\n",
        "    channels_sum, channels_squared_sum, num_batches = 0, 0, 0\n",
        "\n",
        "    for data, _ in loader:\n",
        "        # Reshape data to be [batch_size, height * width]\n",
        "        data = data.view(data.size(0), -1)\n",
        "\n",
        "        # Update sums and sum of squares\n",
        "        channels_sum += torch.sum(data, dim=1).sum()\n",
        "        channels_squared_sum += torch.sum(data ** 2, dim=1).sum()\n",
        "        num_batches += data.size(0)\n",
        "\n",
        "    # Calculate mean and standard deviation\n",
        "    mean = channels_sum / (num_batches * 28 * 28)  # 28*28 is the size of each image\n",
        "    std = ((channels_squared_sum / (num_batches * 28 * 28)) - (mean ** 2)) ** 0.5\n",
        "\n",
        "    print(f\"Mean: {mean}\")\n",
        "    print(f\"Standard Deviation: {std}\")"
      ],
      "metadata": {
        "id": "0xj4gCFMX96i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader_mean_std(train_loader)\n",
        "loader_mean_std(test_loader)"
      ],
      "metadata": {
        "id": "0j2uRpdmZSuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFN2bSnmluYF"
      },
      "source": [
        "### 2.1.2 Training an NN\n",
        "We've got our data, so now we need to build a model. Our strategy will be similar to the hand-built NN in Part 1, except this time we're going to use a more complicated model.\n",
        "\n",
        "We'll use the $\\mathrm{ReLU}(z)=\\max(0,z)$ activation function and 2 hidden layers of size $128$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27hq9lQTKGTD"
      },
      "source": [
        "To do so, we're going to use the following `PyTorch` objects belonging to the `torch.nn` class:\n",
        "\n",
        "- `Linear` creates weight and bias matrices populated with learnable parameters.\n",
        "- `ReLU` implements the ReLU activation function.\n",
        "- `Sequential` is a container for modules like `Linear` and `ReLU`. Once you have a `Sequential` object, it will apply all its modules in sequence, producing an output for you.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxYJOOCeKN2C"
      },
      "source": [
        "We're going to use these 3 ingredients in the cells below to create the following NN architecture:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Input}:&\\qquad \\mathbf{x}_0\\in\\mathbb{R}^{784}\\\\\n",
        "\\text{1st Preactivations}:&\\qquad  \\mathbf{z}_1 = \\mathbf{W}_1\\mathbf{x}_0 + \\mathbf{b}_1\\\\\n",
        "\\text{1st Postactivations}:&\\qquad \\sigma_1 = \\mathrm{ReLU}(\\mathbf{z}_1)\\\\\n",
        "\\text{2nd Preactivations}:&\\qquad \\mathbf{z}_2 = \\mathbf{W}_2\\sigma_1+ \\mathbf{b}_2\\\\\n",
        "\\text{2nd Postactivations}:&\\qquad \\sigma_2=\\mathrm{ReLU}(\\mathbf{z}_2)\\\\\n",
        "\\text{Output}:&\\qquad \\mathbf{W}_3\\sigma_2 + \\mathbf{b}_3\\in\\mathbb{R}^{10}\n",
        "\\end{align}\n",
        "\n",
        "Our network is a function $f:\\mathbb{R}^{784}\\rightarrow\\mathbb{R}^{10}$, i.e. its outputs will correspond to the probabilities that the $784$-dimensional input is each digit between 0 and 9, where these ten categories are encoded in a one-hot manner.\n",
        "\n",
        "Note that there's no output (softmax) activation function because `PyTorch` will apply it for you when you use its implementation of cross entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHPOiZNgls2c"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import time\n",
        "from torch.nn import Sequential, Linear, ReLU, CrossEntropyLoss\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptyX1xbKlRcC"
      },
      "outputs": [],
      "source": [
        "model = Sequential(\n",
        "    Linear(784, 128), # maps input_dim 784 to hidden_dim 128\n",
        "    ReLU(),           # 1st hidden layer has dimension 128\n",
        "    Linear(128, 128), # maps hidden_dim 128 to hidden_dim 128\n",
        "    ReLU(),           # 2nd hidden layer has dim 128\n",
        "    Linear(128, 10)   # maps hidden_dim 128 to output_dim 10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cWJaQR07AkN"
      },
      "source": [
        "Now that we have a model, we can create a loss function to optimize. As before, we'll choose to optimize Cross Entropy since we've got another multi-class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO6vutPHpK-t"
      },
      "outputs": [],
      "source": [
        "loss_fn = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9emNLOTu8Bhi"
      },
      "source": [
        "Previously, we applied gradient descent to train our network. We're going to do something similar here by applying *stochastic gradient descent* (SGD).\n",
        "\n",
        "Here, stochastic means that instead of calculating full gradients (which is expensive), we're going to estimate the gradients by randomly selecting subsets of data.\n",
        "\n",
        "The learning rate, or `lr` below, controls the speed of learning and needs to be carefully tuned in practice.\n",
        "\n",
        "We've also introduced another hyperparameter called `momentum`. In a nutshell, momentum means that the minimization \"picks up speed\" gradient descent - if a direction is consistently favorable, the optimization will begin to build speed in that direction. It also acts as a low-pass filter, by smoothing out the gradient updates.\n",
        "\n",
        "This is generally a good thing to do in practice, as you can \"run past\" noisy fluctuations or flat regions of the loss landscape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufHfm-aPpQgo"
      },
      "outputs": [],
      "source": [
        "optimizer = SGD(model.parameters(), lr=0.005, momentum=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DvcbQSt9XYc"
      },
      "source": [
        "Now we need to train our model. To do so, we're going to run a standard `PyTorch` train step repeatedly over all the training data. In practice, you'll usually grab a similar train function from the `PyTorch` documentation pages and then adapt it to your own needs. The train step has several key ingredients, which are heavily commented in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3ZLChzMnkpi"
      },
      "outputs": [],
      "source": [
        "def train_step(model, train_loader, optimizer, loss_fn):\n",
        "  # track the total loss and the number N of images processed\n",
        "  total_loss, N = 0, 0\n",
        "\n",
        "  # set the model in train mode, which tells PyTorch to track gradients\n",
        "  model.train()\n",
        "\n",
        "  # loop over batches of images (batch_size x 784) and labels (784)\n",
        "  t0 = time.time()\n",
        "  for data in train_loader:\n",
        "\n",
        "    # unpack the data in the batch\n",
        "    (images, labels) = data\n",
        "\n",
        "    # zero out any gradients calculated during the last batch\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # use the model to predict labels for each image\n",
        "    y_pred = model(images)\n",
        "\n",
        "    # calculate the cross entropy loss between the prediction and truth\n",
        "    loss = loss_fn(y_pred, labels)\n",
        "\n",
        "    # compute the gradients of the loss function\n",
        "    loss.backward()\n",
        "\n",
        "    # use the gradients to update the model's learnable parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # accumulate some statistics\n",
        "    total_loss += loss.item()\n",
        "    N+=len(labels)\n",
        "\n",
        "  return total_loss/N, time.time()-t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmCmBgecAQ1O"
      },
      "source": [
        "With our train step in hand, now we just need to run a training loop over several epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKfdMgCmAXT1"
      },
      "outputs": [],
      "source": [
        "# training sequence\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "  epoch_loss, epoch_time = train_step(model, train_loader, optimizer, loss_fn)\n",
        "  print(f\"Epoch {epoch}: loss={epoch_loss:.4f}, time={epoch_time:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XZulSVUA8Mo"
      },
      "source": [
        "You should notice that the loss steadily decreases from epoch to epoch. Each epoch should take about 10 seconds. If this feels slow, don't worry - we'll speed it up in subsequent cells.\n",
        "\n",
        "Now that our model has been trained, it's time to evaluate its performance.\n",
        "\n",
        "Ideally, we hope that it has learned to recognize images from the train set in a general way, i.e. that it hasn't just memorized the train data (this is called **overfitting**).\n",
        "\n",
        "To test the model's **generalization** - that is, its performance on unseen data - we will apply it to the sample of **test data**, which it has so far never seen.\n",
        "\n",
        "In doing so, we need to write a new test step. This will look similar to the train step, except now we're *not* going to calculate gradients or update the model's learnable parameters. We encode this explicitly by writing `model.eval()` and `with torch.no_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3TDkLkIp7b3"
      },
      "outputs": [],
      "source": [
        "def test_step(model, test_loader, loss_fn):\n",
        "  # statistics to accumulate\n",
        "  total_loss, total_correct, N = 0, 0, 0\n",
        "\n",
        "  # tell torch not to track gradients or update the model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # loop over all batches in the test data\n",
        "    for data in enumerate(test_loader):\n",
        "\n",
        "      # unpack the batch of data\n",
        "      idx, (images, labels) = data\n",
        "\n",
        "      # calculate the model's prediction\n",
        "      y_pred = model(images)\n",
        "\n",
        "      # compute the loss and store relevant statistics\n",
        "      loss = loss_fn(y_pred, labels)\n",
        "      total_loss += loss.item()\n",
        "      total_correct += sum(y_pred.argmax(1)==labels)\n",
        "      N+=len(labels)\n",
        "\n",
        "  return total_loss/N, total_correct/N\n",
        "\n",
        "# evaluate the model\n",
        "loss, acc = test_step(model, test_loader, loss_fn)\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"Average Loss: {loss:.3f}\")\n",
        "print(f\"Classification Accuracy: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4fr2bx-CSo0"
      },
      "source": [
        "Hey, 97% is pretty good considering we just treated the images as vectors, effectively disregarding any local spatial information inside of them.\n",
        "\n",
        "More complicated architectures like convolutional neural networks (CNNs) can leverage this localized information to make even better predictions on MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kryb6_EF8y5T"
      },
      "source": [
        "## 2.2 Faster Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa_NR3J7uOxY"
      },
      "source": [
        "You may have noticed that training the model is somewhat slow. We can speed it up significantly by using an accelerated hardware device called a **graphics processing unit (GPU)**. Navigate to `Edit` $\\rightarrow$ `Notebook Settings` and change the hardware accelerator to a GPU. Check that torch is able to communicate with the GPU by running the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G6IhZQB7U81"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYDCjrno7dX2"
      },
      "source": [
        "You should see `device(type='cuda')` if torch has successfully interfaced with the GPU. We can send objects to the GPU if we want to do expensive operations (e.g. large matrix multiplications) on them. The below cell shows an example of sending a tensor to the GPU using the `.to(device)` syntax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cL9ap6G7cTH"
      },
      "outputs": [],
      "source": [
        "t = torch.tensor([1,2,3,4], dtype=torch.float).to(device)\n",
        "t.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ggaIGC4ERwv"
      },
      "source": [
        "It turns out that our code in 2.1 is quite slow because the default `MNIST Dataset` object carries around a lot of unnecessary baggage required to work with images. Since we're just flattening the image and treating it as a vector of numbers, we can actually do a bit better by loading all the data onto the GPU as a list of vectors. Let's do so using a modified version of [this FastMNIST script](https://github.com/y0ast/pytorch-snippets/tree/main/fast_mnist). You don't need to pay too much attention to the code below - just check that we're flattening, re-scaling, and normalizing the data before loading it all into the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHn5_VgYERGs"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "\n",
        "class FastMNIST(MNIST): # build our new class on top of the old one\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        # flatten, re-scale, normalize the data\n",
        "        self.data = self.data.unsqueeze(1).float().div(255)\n",
        "        self.data = self.data.sub_(0.1307).div_(0.3081)\n",
        "        self.data = self.data.view(len(self.data), 28*28)\n",
        "\n",
        "        # load all the images and labels onto the GPU in advance\n",
        "        self.data, self.targets = self.data.to(device), self.targets.to(device)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "        return (img, target)\n",
        "\n",
        "# create our new FastMNIST dataset\n",
        "train_data = FastMNIST(\"data\", train=True, download=True)\n",
        "test_data = FastMNIST(\"data\", train=False, download=True)\n",
        "\n",
        "# this works just like our last dataset except everything is now on a GPU\n",
        "train_loader = DataLoader(train_data, batch_size=128, num_workers=0, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=128, num_workers=0, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_G6unj6DRN8"
      },
      "source": [
        "Let's start our training procedure over. Above, we have code for the `train_step` and `test_step`, so let's define a new model and train it again by minimizing `CrossEntropyLoss` with the `SGD` optimization algorithm. This time, you'll see that it trains *much* faster with each epoch taking around 1 second."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JHqlvUGFxq7"
      },
      "outputs": [],
      "source": [
        "model = Sequential(\n",
        "    Linear(784, 128),\n",
        "    ReLU(), # hidden layer has dimension 32\n",
        "    Linear(128, 128),\n",
        "    ReLU(),\n",
        "    Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = SGD(model.parameters(), lr=0.005, momentum=0.8)\n",
        "\n",
        "# training sequence\n",
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "  epoch_loss, epoch_time = train_step(model, train_loader, optimizer, loss_fn)\n",
        "  print(f\"Epoch {epoch}: loss={epoch_loss:.4f}, time={epoch_time:.4f}\")\n",
        "\n",
        "# evaluate the model\n",
        "loss, acc = test_step(model, test_loader, loss_fn)\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"Average Loss: {loss:.3f}\")\n",
        "print(f\"Classification Accuracy: {acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsPuf0_LuOtY"
      },
      "source": [
        "**EXERCISE 6**: Now that our training is sped up, we spend a bit more time tuning our model and training scheme. So far, we've picked the hyperparameters (e.g. learning rate and momentum in the optimizer, batch size, the size of the hidden layers, the number of hidden layers) somewhat randomly. Try varying them a bit - how does the model performance change? Can you get a better classification accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GozYU8ZvLm-"
      },
      "source": [
        "## 2.3 Autograd (Optional)\n",
        "\n",
        "You may have noticed that we didn't ever have to calculate any gradients by hand.  That's one of the main benefits of modern deep learning libraries - they track gradients for you!\n",
        "\n",
        "Let's take a look at how this works. Consider a test function\n",
        "\n",
        "$$f(x, y) = 5x^2 + y$$\n",
        "\n",
        "In practice, $f$ would be the loss function you're trying to optimize, but here we're just using an example we can easily calculate by hand. The gradient of $f$ is\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial x} = 10x$$\n",
        "\n",
        "$$\\frac{\\partial f}{\\partial y} = 1$$\n",
        "\n",
        "Now that we've calculated these derivatives by hand, let's see how this works in `PyTorch`. First, let's set $x=2$ and $y=10$ as torch tensors that have the `requires_grad` flag labeled `True`. That means that the derivatives of $x$ and $y$ will be stored as we use them to compute $f$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FoCvGQetJUI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(2., requires_grad=True)\n",
        "y = torch.tensor(10., requires_grad=True)\n",
        "\n",
        "f = 5*x**2 + y\n",
        "f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MslBiOgD4gt"
      },
      "source": [
        "So far, so good; when `f(2, 10) = (5)(2^2) + 10 = 30`.\n",
        "\n",
        "Notice that the `grad_fn` of $f$ is `AddBackward`.\n",
        "\n",
        "`PyTorch` saw that we added `x` and `y` and stored that operation.\n",
        "\n",
        "We ask for the gradient of `f` by calling `f.backward()`.\n",
        "\n",
        "This operation will check the history of operations used to calculate `f` - here only `AddBackward` - and store the relevant derivatives in each object used to compute `f`. Let's see this explicitly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcVFz8-rvKm0"
      },
      "outputs": [],
      "source": [
        "f.backward() # backpropagation, accumulates the gradients of f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vtzhj3P7xVvm"
      },
      "outputs": [],
      "source": [
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3eqhvQ3yMiR"
      },
      "outputs": [],
      "source": [
        "y.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXGW-wYIFdFW"
      },
      "source": [
        "Awesome, right? The gradients stored at `x` and `y` are just\n",
        "\n",
        "$\\frac{\\partial f}{\\partial x}|_{x=2}=20$\n",
        "\n",
        "$\\frac{\\partial f}{\\partial y}|_{y=10}=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZxIBXw0Fcef"
      },
      "source": [
        "Now, imagine a scenario where you've composed a function from many repeated operations, for example the chain of matrix multiplications/additions and non-linearities that make up a neural network.\n",
        "\n",
        "When you ask for the gradient of the loss function with `loss.backward()` (see the `train_step` code above), you're asking `PyTorch` to apply the chain rule over a long history of mathematical operations, ultimately recovering gradients at each learnable parameter in the NN. These gradients are used to update the learnable parameters.\n",
        "\n",
        "\n",
        "\n",
        "> **EXERCISE 6**: Implement a single-layer NN by hand using the tensors provided below. Don't use any activation function (or, equivalently, use the identity function as your activation). Use the dummy data below to calculate the gradients associated with the mean square error (MSE) loss function:\n",
        "\n",
        "$$ L_{MSE}(\\mathbf{y}, \\widehat{\\mathbf{y}}) = \\sum_{j=1}^{d_\\mathrm{out}}(\\mathbf{y}_{j}-\\widehat{\\mathbf{y}}_{j})^2$$\n",
        "\n",
        "Check that these gradients make sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5fC7MQ8FbdB"
      },
      "outputs": [],
      "source": [
        "# input data point\n",
        "x = torch.tensor([[1], [2]], dtype=torch.float, requires_grad=True)\n",
        "y = torch.tensor([[6, 6]], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "# NN ingredients\n",
        "W_1 = torch.tensor([[1, 1], [1, 1]], dtype=torch.float, requires_grad=True)\n",
        "b_1 = torch.tensor([[1, 0]], dtype=torch.float, requires_grad=True)\n",
        "W_2 = torch.tensor([[1, 1]], dtype=torch.float, requires_grad=True)\n",
        "b_2 = torch.tensor([[0, 1]], dtype=torch.float, requires_grad=True)\n",
        "\n",
        "# apply the above NN ingredients to produce a prediction\n",
        "y_pred = W_2 @ (W_1 @ x + b_1) + b_2\n",
        "print(y_pred)\n",
        "\n",
        "# MSE\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Apply MSE, calculate the loss,\n",
        "# and check the gradients accumulated in x\n",
        "loss = loss_fn(y_pred, y)\n",
        "loss.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qsnLsrBI9ZB"
      },
      "source": [
        "\\begin{align*}\\frac{\\partial L_{MSE}}{\\partial \\mathbf{x}} &=\n",
        "\\frac{\\partial L_{MSE}}{\\partial \\hat{\\mathbf{y}}}\\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{x}}\\\\ &= 2(\\,(\\hat{y}_{0}-y_{0}) + (\\hat{y}_{1}-y_{1}) \\,) \\mathbf{W}_2 \\mathbf{W}_1 \\\\\n",
        "&=\n",
        "2(2+1)\\begin{bmatrix}1&1\\end{bmatrix}\\begin{bmatrix} 1&1\\\\1&1\\end{bmatrix} \\\\\n",
        "&= \\begin{bmatrix} 12&12 \\end{bmatrix}\n",
        "\\end{align*}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Solutions to Exercises"
      ],
      "metadata": {
        "id": "SCv6DPAqJz1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 1**:  \n",
        "\n",
        "2x500 (from W1) + 500 (from b1) + 500 * 2 (from W2) + 2 (from b2) = 2502."
      ],
      "metadata": {
        "id": "Eu82JvM-J4y0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 2**:\n",
        "\n",
        "\n",
        "Softmax is sensitive to the temperature of its inputs; that is, the gap between 0.5 and 1 is 0.5, much smaller than the gap between 5 and 10, which is 5. Softmax probabilities change to reflect this difference, whereas standard normalization remains the same."
      ],
      "metadata": {
        "id": "eIU9lYkrKITX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "import torch\n",
        "\n",
        "a = torch.Tensor([0.5, 1]) # standard normalization [0.5 / 1.5, 1 / 1.5] = [0.33, 0.67]\n",
        "b = torch.Tensor([5, 10]) # standard normalization [5 / 15, 10 / 15] = [0.33, 0.67]\n",
        "\n",
        "softmax(a), softmax(b)"
      ],
      "metadata": {
        "id": "LeMFRWJFKJ1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISE 3**:\n",
        "\n",
        "BCE is just the 2-class special case of our more general cross entropy loss function. To be clear, we applied softmax to our one-hot outputs before computing the cross entropy loss:\n",
        "\n",
        "$$ p_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}\\qquad p_2 = \\frac{e^{z_2}}{e^{z_1} + e^{z_2}}$$\n",
        "\n",
        "where $z_1$ and $z_2$ are the output **logits** of the model. If we instead have a model that computes a single output logit $z:=z_2-z_1$, as we did yesterday when studying BCE, we should instead apply the sigmoid function:\n",
        "\n",
        "$$ p_2 = \\sigma(z) = \\frac{1}{1+e^{-(z_2-z_1)}} = \\frac{e^{z_2}}{e^{z_1} + e^{z_2}}$$\n",
        "\n",
        "and, implicitly,\n",
        "\n",
        "$$ p_1 = 1 - p_2 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2}}.$$\n",
        "\n"
      ],
      "metadata": {
        "id": "OBNhpRveO1Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EXERCISES 4 and 5**:\n",
        "\n",
        "The provided code is all you really need. Note that increasing the width may slightly help the performance, but that increasing the learning rate too much destroys the model's ability to fit the data!"
      ],
      "metadata": {
        "id": "GOeJbSRTVUWG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FuOQULmty2c0",
        "spXRy6Jd8JwL",
        "aPhFRtDFLscd",
        "SCv6DPAqJz1J"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}